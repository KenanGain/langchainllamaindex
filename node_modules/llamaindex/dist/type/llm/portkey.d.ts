import { BaseLLM, type ChatMessage, type ChatResponse, type ChatResponseChunk, type LLMChatParamsNonStreaming, type LLMChatParamsStreaming, type LLMMetadata } from "@llamaindex/core/llms";
import { Portkey as OrigPortKey } from "portkey-ai";
type PortkeyOptions = ConstructorParameters<typeof OrigPortKey>[0];
export declare class PortkeySession {
    portkey: OrigPortKey;
    constructor(options?: PortkeyOptions);
}
/**
 * Get a session for the Portkey API. If one already exists with the same options,
 * it will be returned. Otherwise, a new session will be created.
 * @param options
 * @returns
 */
export declare function getPortkeySession(options?: PortkeyOptions): PortkeySession;
export declare class Portkey extends BaseLLM {
    apiKey?: string | undefined;
    baseURL?: string | undefined;
    session: PortkeySession;
    constructor(init?: Partial<Portkey> & PortkeyOptions);
    get metadata(): LLMMetadata;
    chat(params: LLMChatParamsStreaming): Promise<AsyncIterable<ChatResponseChunk>>;
    chat(params: LLMChatParamsNonStreaming): Promise<ChatResponse>;
    streamChat(messages: ChatMessage[], params?: Record<string, any>): AsyncIterable<ChatResponseChunk>;
}
export {};
